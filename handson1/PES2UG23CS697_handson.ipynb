{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "elMF4hc-q6R3",
        "outputId": "74029f43-518a-4c52-ec55-c930ef0e6b1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import nltk\n",
        "from transformers import pipeline, set_seed, GPT2Tokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WF4flMwxrjSk",
        "outputId": "1338aa8f-0b1f-4d90-d80b-05c4629e52e4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "file_path=\"unit 1.txt\"\n",
        "try:\n",
        "    with open(file_path,'r', encoding=\"utf-8\") as f:\n",
        "        text=f.read()\n",
        "    print(\"file loaded successfully\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: The file {file_path} was not found.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5fETl81vrEYp",
        "outputId": "c0f7236e-9205-4a8d-9694-7cf17963f855"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "file loaded successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "j6n33RzZsQGn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--Data Preview--\")\n",
        "print(text[:500]+\"...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jNgzzc5er8qw",
        "outputId": "75e5bcbe-36a0-4192-ca6e-ddb2fb75f7fb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--Data Preview--\n",
            "Generative AI and Its Applications: A Foundational Briefing\n",
            "\n",
            "Executive Summary\n",
            "\n",
            "This document provides a comprehensive overview of Generative AI, synthesizing foundational concepts, technological underpinnings, and practical applications as outlined in the course materials from PES University. Generative AI represents a transformative subset of Artificial Intelligence focused on creating novel content, a capability primarily driven by the advent of Large Language Models (LLMs). The evolution of ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed(42)"
      ],
      "metadata": {
        "id": "2j9ScnEQsqER"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt=\"Generative AI is a revolutionary technology that\"\n"
      ],
      "metadata": {
        "id": "Jh0aKNAKsvkh"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the pipeline with the specific model\n",
        "fast_generator = pipeline('text-generation', model='distilgpt2')\n",
        "\n",
        "# Generate text\n",
        "output_fast = fast_generator(prompt, max_length=50, num_return_sequences=1)\n",
        "print(output_fast[0]['generated_text'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qqN8BKuWtYQT",
        "outputId": "eb3976ad-cf80-4314-e55e-bb84a84df9de"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Device set to use cpu\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generative AI is a revolutionary technology that is designed to work with existing AI systems. It has been developed by the University of California, Berkeley. Its research team is the leading developer of AI software and its use is limited to AI and AI systems.\n",
            "\n",
            "\n",
            "The research team led by Professor Daniel Kranz, from the University of California, Berkeley, has developed a program to learn how to use the AI to improve the performance of the software. It has been developed by the University of California, Berkeley. Its research team is the leading developer of AI software and its use is limited to AI and AI systems. It is a top-selling research computer software company, and is a top-selling research computer software company.\n",
            "The research team developed the program to learn how to use the AI to improve the performance of the software. It has been developed by the University of California, Berkeley, Berkeley, and is a top-selling research computer software company. The research team developed the program to learn how to use the AI to improve the performance of the software. It has been developed by the University of California, Berkeley, and is a top-selling research computer software company.\n",
            "The research team developed the program to learn how to use the AI to improve the performance of the software. It has been developed by\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "smart_generator = pipeline('text-generation', model='gpt2')\n",
        "\n",
        "output_smart = smart_generator(prompt, max_length=50, num_return_sequences=1)\n",
        "print(output_smart[0]['generated_text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lwKOYnnzudJH",
        "outputId": "7961a3fd-270d-479b-8319-ba1ef668dca0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generative AI is a revolutionary technology that allows users to build AI that can help solve complex problems. It brings together hundreds of different approaches to solve problems, from solving complex problems in a laboratory to solving complex problems in a city. The technology allows users to build a computer that makes decisions based on user input, not on intuition.\n",
            "\n",
            "The AI is a model of human intelligence, and has many aspects that are similar to artificial intelligence. It can learn from humans, and it can adapt to the environment. It can learn by experimenting with new ways of thinking, and it can learn by learning from its own experience.\n",
            "\n",
            "It is the main driving force behind the new Artificial Intelligence, and the AI is very important to the success of AI. The new AI is designed to work out problems that need to be solved in a way that is easy to understand and solve, and that is flexible enough to be easily adaptable to different environments.\n",
            "\n",
            "The AI is designed to be scalable and adaptable to different environments. It can be used to solve complex problems without relying on humans. It can be used to build a solution that is very quickly scalable, scalable, inexpensive, and adaptable to different environments.\n",
            "\n",
            "The new AI is designed to work out problems that need to be solved in a way that is\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#nlp\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "sample_sentence = \"Transformers revolutionized NLP.\"\n",
        "tokens=tokenizer.tokenize(sample_sentence)\n",
        "print(f\"Tokens: {tokens}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P37i_pO5vgyE",
        "outputId": "20829bb8-e90a-4cbf-8279-532bdcaee107"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['Transform', 'ers', 'Ġrevolution', 'ized', 'ĠN', 'LP', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token_ids=tokenizer.convert_tokens_to_ids(tokens)\n",
        "print(f\"Token IDs: {token_ids}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "br8wAdXXvrrV",
        "outputId": "9724249a-0169-405d-ca54-8f9ab20551ad"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token IDs: [41762, 364, 5854, 1143, 399, 19930, 13]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('punkt_tab', force=True)\n",
        "nltk.download('punkt', force=True)\n",
        "nltk.download('averaged_perceptron_tagger', force=True)\n",
        "\n",
        "print(\"Downloads complete\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mAecr1VZxdb8",
        "outputId": "8b8e12ee-006c-475d-8f29-1722ccc6dd24"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloads complete\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pos_tagger = pipeline(\"token-classification\",model=\"dslim/bert-base-NER\",aggregation_strategy=\"simple\")\n",
        "pos_tagger(sample_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3zZiyw6Exvw3",
        "outputId": "f6aad49b-3f7c-4f5c-c5e4-8b908c41e565"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'entity_group': 'ORG',\n",
              "  'score': np.float32(0.9368283),\n",
              "  'word': 'Transformers',\n",
              "  'start': 0,\n",
              "  'end': 12},\n",
              " {'entity_group': 'ORG',\n",
              "  'score': np.float32(0.95586836),\n",
              "  'word': 'NLP',\n",
              "  'start': 28,\n",
              "  'end': 31}]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger_eng',quiet=True)\n",
        "pos_tags=nltk.pos_tag(nltk.word_tokenize(sample_sentence))\n",
        "print(f\"POS Tags: {pos_tags}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g8ITF2V9um9c",
        "outputId": "d42c5799-b203-47af-dfe2-f9d8091618b8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POS Tags: [('Transformers', 'NNS'), ('revolutionized', 'VBD'), ('NLP', 'NNP'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ner_pipeline=pipeline(\"ner\",model=\"dbmdz/bert-large-cased-finetuned-conll03-english\",aggregation_strategy=\"simple\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mb2NeJekyJf-",
        "outputId": "15e23bf6-4612-4ef5-8633-28338fd3a0a0"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "snippet=text[:1000]\n",
        "entities=ner_pipeline(snippet)\n",
        "print(f\"{\"Entity\":<20} | {'Type':<10} | {'Score':<5}\")\n",
        "print(\"-\"*45)\n",
        "for entity in entities:\n",
        "  if entity['score']>0.90:\n",
        "    print(f\"{entity['word']:<20} | {entity['entity_group']:<10} | {entity['score']:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WEXUlVMc0TNP",
        "outputId": "8f0cee01-4e5e-44a4-b03d-f0aedeaca59a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entity               | Type       | Score\n",
            "---------------------------------------------\n",
            "AI                   | MISC       | 0.98\n",
            "PES University       | ORG        | 0.99\n",
            "AI                   | MISC       | 0.98\n",
            "Large Language Models | MISC       | 0.91\n",
            "LLMs                 | MISC       | 0.90\n",
            "Transformer          | MISC       | 0.99\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transformer_section=\"The introduction of the Transformer architecture marked a major turning point in the field of artificial intelligence and natural language processing. Proposed in the 2017 paper “Attention Is All You Need,” the Transformer replaced older sequence models like Recurrent Neural Networks (RNNs) and Long Short-Term Memory networks (LSTMs), which processed words one at a time and struggled with long-range dependencies.\""
      ],
      "metadata": {
        "id": "YDkNOZdC0fY3"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fast_sum = pipeline(\"summarization\", model=\"sshleifer/distilbart-cnn-12-6\")\n",
        "res_fast = fast_sum(transformer_section, max_length=60, min_length=30, do_sample=False)\n",
        "print(res_fast[0]['summary_text'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NHZTqRMx2DvI",
        "outputId": "3cbe50b5-bc79-4eee-de58-48adb2f41f68"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " The introduction of the Transformer architecture marked a major turning point in the field of artificial intelligence and natural language processing . Proposed in the 2017 paper “Attention Is All You Need,” the new architecture replaced older sequence models like Recurrent Neural Networks (RNNs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "smart_sum = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "res_smart = smart_sum(transformer_section, max_length=60, min_length=30, do_sample=False)\n",
        "print(res_smart[0]['summary_text'])\n"
      ],
      "metadata": {
        "id": "bVDrATVr2Vob",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e77c6c06-26c7-457f-9fc3-8a9a559bacc7"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The introduction of the Transformer architecture marked a major turning point in the field of artificial intelligence and natural language processing. The Transformer replaced older sequence models like Recurrent Neural Networks (RNNs) and Long Short-Term Memory networks (LSTMs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qa_pipeline = pipeline(\"question-answering\", model=\"distilbert-base-cased-distilled-squad\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2H4tISdkvWKe",
        "outputId": "7dbd9679-02f1-4ff2-9291-f6b68bc2a958"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "questions = [\n",
        "    \"What is the evaluation policy?\",\n",
        "    \"What are the risks of using Generative AI?\"\n",
        "]\n",
        "\n",
        "for q in questions:\n",
        "    res = qa_pipeline(question=q, context=text[:5000])\n",
        "    print(f\"\\nQ: {q}\")\n",
        "    print(f\"A: {res['answer']}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "udJEzcHcvau1",
        "outputId": "f476302c-6085-46e9-bee3-1026d05dd71e"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Q: What is the evaluation policy?\n",
            "A: specific structure\n",
            "\n",
            "Q: What are the risks of using Generative AI?\n",
            "A: data privacy, intellectual property, and academic integrity\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mask_filler = pipeline(\"fill-mask\", model=\"bert-base-uncased\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mvAxZ223vfSU",
        "outputId": "35491350-1c08-4527-b940-1cee065340a0"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "masked_sentence = \"The goal of Generative AI is to create new [MASK].\"\n",
        "preds = mask_filler(masked_sentence)\n",
        "\n",
        "for p in preds:\n",
        "    print(f\"{p['token_str']}: {p['score']:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eIxD4eAevhUS",
        "outputId": "c1b1c6a1-6b44-40a1-8c63-6af85859da81"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "applications: 0.06\n",
            "ideas: 0.05\n",
            "problems: 0.05\n",
            "systems: 0.04\n",
            "information: 0.03\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zNQZO1K7wEMg"
      },
      "execution_count": 22,
      "outputs": []
    }
  ]
}